<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="Content-Type" content="text/html" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- Icon start -->
        <link rel="icon" href="../images/icons/IconSheet.svg#browserlogo">
        <link rel="apple-touch-icon" href="../images/icons/IconSheet.svg#browserlogo">
        <link rel="shortcut icon" href="../images/icons/IconSheet.svg#browserlogo" />
        <link rel="mask-icon" href="../images/icons/IconSheet.svg#browserlogo" />
        <!-- Icon end -->
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script defer src="https://www.googletagmanager.com/gtag/js?id=G-2W1VXE5GSE"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-2W1VXE5GSE');
        </script>
        <!-- Google Analytics TAG END  ---------------------->
        <!-- NO JS Behavior START -->
        <noscript>
            <style>
                nav.sidenav {display:none;}
                li.nav-item{display:none;}
            </style>
        </noscript>
         <!-- NO JS Behavior END -->

        <title>Jason Yang - Linear Regression and Matrices</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>

    <body>
        <!-- Side navigation start -->
        <nav class="sidenav">
            <li class="logo">
                <a href="#" class="nav-link">
                    <span class="link-text logo-text">Jason</span>
                    <svg><use href="../images/icons/IconSheet.svg#sidebardod"></use></svg>
                </a>
            </li>
        
            
            <li class="nav-item">
                <a href="../tags/mathcs.html" class="nav-link">
                    <svg><use href="../images/icons/IconSheet.svg#lambda"></use></svg>
                    <span class="link-text">Math/CS</span>
                </a>
            </li>
                
            <li class="nav-item">
                <a href="../tags/prog.html" class="nav-link">
                    <svg><use href="../images/icons/IconSheet.svg#progcode"></use></svg>
                    <span class="link-text">Prog</span>
                </a>
            </li>


            <li class="nav-item">
                <a href="../tags/AI.html" class="nav-link">
                    <svg><use href="../images/icons/IconSheet.svg#AIbrain"></use></svg>
                    <span class="link-text">ML/AI</span>
                </a>
            </li>

            <li class="nav-item">
                <a href="../tags/tech.html" class="nav-link">
                    <svg><use href="../images/icons/IconSheet.svg#hardware"></use></svg>
                    <span class="link-text">Tech</span>
                </a>
            </li>

            <li class="nav-item">
                <a href="../tags/musings.html" class="nav-link">
                    <svg><use href="../images/icons/IconSheet.svg#thinker"></use></svg>
                    <span class="link-text">Musings</span>
                </a>
            </li>


            <li class="nav-item">
                <a href="#" class="nav-link">
                    <svg><use href="../images/icons/IconSheet.svg#react"></use></svg>
                    <span class="link-text">React</span>
                </a>
            </li>
        </nav>
        <!-- Side navigation end -->
        <div id="header">
            <div id="logo">
                <a href="../">Jason Yang</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Linear Regression and Matrices</h1>
            

            <div class="info">
    Posted on July  7, 2016
    
</div>
<div class="info">
    
    Tags: <a title="All pages tagged 'mathcs'." href="../tags/mathcs.html">mathcs</a>, <a title="All pages tagged 'puremath'." href="../tags/puremath.html">puremath</a>, <a title="All pages tagged 'recursion'." href="../tags/recursion.html">recursion</a>
    
</div>
<div id="TOC"><ul>
<li><a href="#load-data"><span class="toc-section-number">1</span> Load Data</a></li>
<li><a href="#simple-linear-regression"><span class="toc-section-number">2</span> Simple Linear Regression</a>
<ul>
<li><a href="#plotting-gotcha-remember-to-addremove-bias-column"><span class="toc-section-number">2.1</span> Plotting Gotcha: Remember to Add/Remove Bias column</a></li>
</ul></li>
<li><a href="#manual"><span class="toc-section-number">3</span> Manual</a>
<ul>
<li><a href="#prob-plot"><span class="toc-section-number">3.1</span> prob-plot</a></li>
</ul></li>
<li><a href="#statsmodels"><span class="toc-section-number">4</span> Statsmodels</a></li>
<li><a href="#scipy"><span class="toc-section-number">5</span> Scipy</a></li>
<li><a href="#numpy"><span class="toc-section-number">6</span> Numpy</a></li>
<li><a href="#torch"><span class="toc-section-number">7</span> Torch</a>
<ul>
<li><a href="#simple-linreg"><span class="toc-section-number">7.1</span> Simple LinReg</a></li>
<li><a href="#layer-nn"><span class="toc-section-number">7.2</span> 1-layer NN</a></li>
<li><a href="#layer-nn-class-style"><span class="toc-section-number">7.3</span> 1 layer NN Class style</a></li>
</ul></li>
<li><a href="#numpy-vs-scipy-vs-torch-vs-statsmodels"><span class="toc-section-number">8</span> Numpy vs Scipy vs Torch vs Statsmodels</a></li>
</ul></div>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span></code></pre></div>
<section id="load-data" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Load Data</h1>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Boston <span class="op">=</span> pd.read_csv(<span class="st">&quot;../dataset/Boston.csv&quot;</span>)</span></code></pre></div>
</section>
<section id="simple-linear-regression" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Simple Linear Regression</h1>
<p>The <code>ISLR2</code> library contains the <code>Boston</code> data set, which records <code>medv</code> (median house value) for <span class="math inline">\(506\)</span> census tracts in Boston. We will seek to predict <code>medv</code> using <span class="math inline">\(12\)</span> predictors such as <code>rmvar</code> (average number of rooms per house), <code>age</code> (average age of houses), and <code>lstat</code> (percent of households with low socioeconomic status).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Boston.head()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Unnamed: 0
</th>
<th>
crim
</th>
<th>
zn
</th>
<th>
indus
</th>
<th>
chas
</th>
<th>
nox
</th>
<th>
rm
</th>
<th>
age
</th>
<th>
dis
</th>
<th>
rad
</th>
<th>
tax
</th>
<th>
ptratio
</th>
<th>
lstat
</th>
<th>
medv
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0.00632
</td>
<td>
18.0
</td>
<td>
2.31
</td>
<td>
0
</td>
<td>
0.538
</td>
<td>
6.575
</td>
<td>
65.2
</td>
<td>
4.0900
</td>
<td>
1
</td>
<td>
296
</td>
<td>
15.3
</td>
<td>
4.98
</td>
<td>
24.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
0.02731
</td>
<td>
0.0
</td>
<td>
7.07
</td>
<td>
0
</td>
<td>
0.469
</td>
<td>
6.421
</td>
<td>
78.9
</td>
<td>
4.9671
</td>
<td>
2
</td>
<td>
242
</td>
<td>
17.8
</td>
<td>
9.14
</td>
<td>
21.6
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
0.02729
</td>
<td>
0.0
</td>
<td>
7.07
</td>
<td>
0
</td>
<td>
0.469
</td>
<td>
7.185
</td>
<td>
61.1
</td>
<td>
4.9671
</td>
<td>
2
</td>
<td>
242
</td>
<td>
17.8
</td>
<td>
4.03
</td>
<td>
34.7
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
0.03237
</td>
<td>
0.0
</td>
<td>
2.18
</td>
<td>
0
</td>
<td>
0.458
</td>
<td>
6.998
</td>
<td>
45.8
</td>
<td>
6.0622
</td>
<td>
3
</td>
<td>
222
</td>
<td>
18.7
</td>
<td>
2.94
</td>
<td>
33.4
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0.06905
</td>
<td>
0.0
</td>
<td>
2.18
</td>
<td>
0
</td>
<td>
0.458
</td>
<td>
7.147
</td>
<td>
54.2
</td>
<td>
6.0622
</td>
<td>
3
</td>
<td>
222
</td>
<td>
18.7
</td>
<td>
5.33
</td>
<td>
36.2
</td>
</tr>
</tbody>
</table>
</div>
<p>To find out more about the data set, we can type <code>?Boston</code>.</p>
<p>We will start by using the <code>lm()</code> function to fit a simple linear regression model, with <code>medv</code> as the response and <code>lstat</code> as the predictor. The basic syntax is {}, where <code>y</code> is the response, <code>x</code> is the predictor, and <code>data</code> is the data set in which these two variables are kept.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Boston[<span class="st">'lstat'</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> Boston[<span class="st">'medv'</span>]</span></code></pre></div>
<section id="plotting-gotcha-remember-to-addremove-bias-column" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Plotting Gotcha: Remember to Add/Remove Bias column</h2>
<p>We calculate best-fit model by appending bias column to X but in plotting we need to remember to remove the bias column.</p>
<ul>
<li><code>Convert_NP</code> will convert panda.Series to np.ndarray</li>
<li><code>X_bias</code>: Will be used for plotting</li>
<li><code>X</code>: Input vector <code>lstat</code> appended with <code>bias</code> column
<ul>
<li><code>bias</code>: Column vector of 1’s
<ul>
<li>This allows us to get the Y-intercept or Beta_0 from linear regression</li>
</ul></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Convert_NP <span class="op">=</span> <span class="kw">lambda</span> g: g.to_numpy().reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>).astype(<span class="st">'float32'</span>) <span class="co">#convert to one column ndarray</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> Convert_NP(x) <span class="co">#shape (560,1)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> Convert_NP(y) <span class="co">#shape (560,1)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X_NoBias <span class="op">=</span> X <span class="co">#USED FOR PLOTTING PURPOSES</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> np.ones((X.shape[<span class="dv">0</span>],<span class="dv">1</span>)).astype(<span class="st">'float32'</span>) <span class="co">#column of 1's represent intercept coefficient</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>X_bias <span class="op">=</span> np.append(X,bias,axis<span class="op">=</span><span class="dv">1</span>) <span class="co">#shape (560,2)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X_bias</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;X-dim: </span><span class="sc">{X.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">Y-dim: </span><span class="sc">{Y.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>X-dim: (506, 2)
Y-dim: (506, 1)</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>x_manual <span class="op">=</span> np.linspace(Y.<span class="bu">min</span>()<span class="op">-</span>Y.std(),Y.<span class="bu">max</span>()<span class="op">+</span>Y.std(), Y.shape[<span class="dv">0</span>])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_NoBias,Y)</span></code></pre></div>
<figure>
<img src="../images/2016-07-07-LinReg/output_9_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
</section>
</section>
<section id="manual" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Manual</h1>
<p><span class="math display">\[ \hat{\beta} = (X^T X)^{-1} X^T Y \]</span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Betas <span class="op">=</span> np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),Y)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Betas: </span><span class="sc">{</span>Betas<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Betas: [[-0.95004904]
 [34.55383   ]]</code></pre>
<p><span class="math display">\[ \hat{Y} = X \hat{\beta} \]</span><br />
<code>medev</code> <span class="math inline">\(=\)</span> <span class="math inline">\(\beta_0 +\)</span> <span class="math inline">\(\beta_1\)</span><code>lstat</code></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>Y_hat <span class="op">=</span> np.dot(X, Betas)  <span class="co">#shape (506,1)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>x_manual <span class="op">=</span> np.linspace(Y_hat.<span class="bu">min</span>()<span class="op">-</span>Y_hat.std(),Y_hat.<span class="bu">max</span>()<span class="op">+</span>Y_hat.std(), Y_hat.shape[<span class="dv">0</span>])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_NoBias,Y)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X_NoBias,Y_hat,color<span class="op">=</span><span class="st">'g'</span>)</span></code></pre></div>
<figure>
<img src="../images/2016-07-07-LinReg/output_15_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<p><span class="math display">\[Residual = e_i = Y_i - \hat{Y}_i \tag{reducible error}\]</span><br />
<span class="math display">\[RSS = \sum_i (Y_i-\hat{Y}_i)^2 = \sum_i e_i^2\]</span><br />
<span class="math display">\[RSS = \sum Residual^2\]</span><br />
<span class="math display">\[MSE = \frac{1}{n} RSS \tag{loss function}\]</span></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>Residual <span class="op">=</span> Y <span class="op">-</span> Y_hat</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>RSS <span class="op">=</span> (Residual<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>Y_hat.shape[<span class="dv">0</span>])<span class="op">*</span>RSS</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> RSS <span class="op">==</span> ((Y<span class="op">-</span>Y_hat)<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>() </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Residual</span><span class="sc">{</span>Residual[:<span class="dv">3</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">RSS: </span><span class="sc">{</span>RSS<span class="sc">}</span><span class="ch">\n</span><span class="ss">MSE: </span><span class="sc">{</span>MSE<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Residual[[-5.822586 ]
 [-4.27038  ]
 [ 3.9748688]]
RSS: 19472.3828125
MSE: 38.48296998517786</code></pre>
<section id="prob-plot" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> prob-plot</h2>
<p>Prob-plot is what ISLR actually uses but misleading labels QQ plot. QQ plot, plots againsts quantiles which are non-negative. We get these quantiles using the inverse CDF.</p>
<p>Prob-plot the theoretical quantiles is plotted on the x(Calculated using Filliben’s estimate) and our data’s z-score is plotting on the y.</p>
<p>Gotcha: Sort the x-axis of your Plot, REMEMBER TO USE PARAM <code>axis=0</code> The x-axis, are the points that are divided into n-quantiles of the bell curve, n being the number of residuals you have. This means that the area under the bell curve bounded between each of the n points are equal. Know that since bell-curve is bigger in the middle means that there are denser points in the middle(less distance) and spread apart at each ends.</p>
<p>Z-score tells how many std-devs we are from the mean, can be positive or negative. Quantile is equal area partition of probability density function.<br />
<code>CDF :: Z-score --&gt; 100-Quantile</code>, note that 100-quantile is just the percentile<br />
<code>InvCDF = stats.norm.ppf :: 100-Quantile --&gt; Z-score</code></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>Resid_normed <span class="op">=</span> Residual<span class="op">/</span>Residual.std()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Filliben(val):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> val.shape[<span class="dv">0</span>]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    fst <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>(<span class="fl">0.5</span><span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>n))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    midd <span class="op">=</span> [(i<span class="op">-</span><span class="fl">0.3175</span>)<span class="op">/</span>(n<span class="op">+</span><span class="fl">0.365</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,n<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    lst <span class="op">=</span> <span class="fl">0.5</span><span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>n)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.append(np.append(fst,midd),lst)</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>Resid_normed.sort(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.append(Resid_normed[<span class="dv">0</span>], Resid_normed[<span class="dv">1</span>]))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>InvertedPercentile <span class="op">=</span> [stats.norm.ppf(i) <span class="cf">for</span> i <span class="kw">in</span> Filliben(Resid_normed)]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(InvertedPercentile,Resid_normed)</span></code></pre></div>
<pre><code>[-2.4449956 -1.6037545]</code></pre>
<figure>
<img src="../images/2016-07-07-LinReg/output_20_2.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>Residual_Standardized <span class="op">=</span> Residual<span class="op">/</span>Residual.std()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># notice the standardized residual is the z-score</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>normdist <span class="op">=</span> np.random.normal( size<span class="op">=</span><span class="dv">506</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Residual_Standardized.mean())</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Residual_Standardized.std())</span></code></pre></div>
<pre><code>1.2740787e-06
1.0</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].scatter(Y_hat,Residual,facecolors<span class="op">=</span><span class="st">&quot;none&quot;</span>,edgecolors<span class="op">=</span><span class="st">&quot;black&quot;</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].set_title(<span class="st">&quot;yhat vs e&quot;</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">#yy = [stats.percentileofscore(Residual_Standardized, a, 'rank') for a in Residual_Standardized]</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">#yy2=np.sort(yy)</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>SortedResidStand <span class="op">=</span> np.sort(Residual_Standardized,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>normdist <span class="op">=</span> np.random.normal( size<span class="op">=</span><span class="dv">506</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>Sortednormdist <span class="op">=</span> np.sort(normdist)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].scatter(Sortednormdist,SortedResidStand)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].scatter(Y_hat,np.sqrt(np.<span class="bu">abs</span>(Residual_Standardized)))</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<figure>
<img src="../images/2016-07-07-LinReg/output_22_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<ul>
<li>yhat vs e(Top Left): used to check if there is a non-linear pattern
<ul>
<li>The data point looks like a curve, clearly not linear</li>
</ul></li>
<li>Normal QQ Plot(Top Right): tells us if residuals fall in a normal dist if it fits the y=x diagonal
<ul>
<li>the top part shows set of points above the y=x diagonal indicating heavy tails/ more chance of seeing extremes</li>
</ul></li>
<li>Scale-Location Plot(Bottom Left): tells us if residuals have non-constant variance (heteroscedasticity).
<ul>
<li>If our data was linear, a constant variance would show a horizontal line pattern</li>
<li>Since our data is not-linear, its hard to tell if the non-horizontal pattern is due to dynamic variance vs non-linearity</li>
</ul></li>
<li>Residual v Leverage Influence Plot(Bottom Right): Leverage points are outliers of the x-axis
<ul>
<li>Leverage points should be removed</li>
<li>Anything in Cook’s distance of 0.5 as shown by boundary is a leverage point to be removed</li>
</ul></li>
</ul>
<p><span class="math display">\[ Variance(\hat{Y}) = \sigma^2 = \frac{\sum_i (\hat{Y}_i-\overline{\hat{Y}}_i)^2}{n}\]</span><br />
<span class="math display">\[ TSS = \sum_i (\hat{Y}_i-\overline{\hat{Y}}_i)^2 \]</span> <span class="math display">\[ Stddev(\hat{Y}) = \sigma\]</span><br />
<span class="math display">\[R^2 = 1-\frac{RSS}{TSS} \]</span></p>
<ul>
<li>TSS is just the numerator of Variance of Predictions</li>
<li>TSS can be considered the background variance of our outputs Centered around the mean</li>
<li>RSS can be considered variance between our datapoint Centered around the best-fit line</li>
<li>RSS/TSS behaves like a normalized measure of how scattered or clumped our points are from our best-fit line.</li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>Var_Yhat <span class="op">=</span> ((Y_hat <span class="op">-</span> Y_hat.mean())<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()<span class="op">/</span>Y_hat.shape[<span class="dv">0</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>Std_Yhat <span class="op">=</span> Var_Yhat<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>TSS <span class="op">=</span> ((Y_hat <span class="op">-</span> Y_hat.mean())<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(Var_Yhat, Y_hat.var())</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(Std_Yhat, Y_hat.std())</span></code></pre></div>
<p><span class="math display">\[SE(\hat{\mu}) = \frac{\sigma^2}{n}\]</span></p>
<ul>
<li>SE of sample mean tells us how far our sample mean is from population mean</li>
</ul>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>SE_Yhat <span class="op">=</span> (Var_Yhat)<span class="op">/</span>Y_hat.shape[<span class="dv">0</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(SE_Yhat)</span></code></pre></div>
<pre><code>0.09078371181201081</code></pre>
<hr />
</section>
</section>
<section id="statsmodels" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Statsmodels</h1>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> sm.OLS(Y,X)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>fit <span class="op">=</span> model.fit()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fit.summary())</span></code></pre></div>
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.544
Model:                            OLS   Adj. R-squared:                  0.543
Method:                 Least Squares   F-statistic:                     601.6
Date:                Fri, 22 Apr 2022   Prob (F-statistic):           5.08e-88
Time:                        10:33:30   Log-Likelihood:                -1641.5
No. Observations:                 506   AIC:                             3287.
Df Residuals:                     504   BIC:                             3295.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1            -0.9500      0.039    -24.528      0.000      -1.026      -0.874
const         34.5538      0.563     61.415      0.000      33.448      35.659
==============================================================================
Omnibus:                      137.043   Durbin-Watson:                   0.892
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              291.374
Skew:                           1.453   Prob(JB):                     5.36e-64
Kurtosis:                       5.319   Cond. No.                         29.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>pp <span class="op">=</span> np.sort(Residual_Standardized,axis<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span> <span class="co"># parameter axis=0 IS IMPORTANT!!</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>sm.qqplot(pp,line<span class="op">=</span><span class="st">'45'</span>,ax<span class="op">=</span>ax[<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>leverage <span class="op">=</span> fit.get_influence().hat_matrix_diag</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>normresid <span class="op">=</span> fit.get_influence().resid_studentized_internal</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>model_cooks <span class="op">=</span> fit.get_influence().cooks_distance[<span class="dv">0</span>]</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].scatter(leverage,Residual_Standardized)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].set_xlim(<span class="dv">0</span>,<span class="bu">max</span>(leverage)<span class="op">+</span><span class="fl">0.01</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>plt.show</span></code></pre></div>
<pre><code>&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;</code></pre>
<figure>
<img src="../images/2016-07-07-LinReg/output_33_1.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
<hr />
</section>
<section id="scipy" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Scipy</h1>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> stats.linregress(x,y)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="ss">Beta_1:</span><span class="ch">\t\t</span><span class="sc">{</span>result<span class="sc">.</span>slope<span class="sc">}</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="ss">Beta_0:</span><span class="ch">\t\t</span><span class="sc">{</span>result<span class="sc">.</span>intercept<span class="sc">}</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="ss">R:</span><span class="ch">\t\t</span><span class="sc">{</span>result<span class="sc">.</span>rvalue<span class="sc">}</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="ss">R^2:</span><span class="ch">\t\t</span><span class="sc">{</span>result<span class="sc">.</span>rvalue<span class="op">**</span><span class="dv">2</span><span class="sc">}</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="ss">Beta_1 pvalue:</span><span class="ch">\t</span><span class="sc">{</span>result<span class="sc">.</span>pvalue<span class="sc">}</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="ss">SE:</span><span class="ch">\t\t</span><span class="sc">{</span>result<span class="sc">.</span>stderr<span class="sc">}</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="ss">Beta_0 SE:</span><span class="ch">\t</span><span class="sc">{</span>result<span class="sc">.</span>intercept_stderr<span class="sc">}</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>Beta_1:		-0.9500493537579908
Beta_0:		34.5538408793831
R:		-0.737662726174015
R^2:		0.5441462975864798
Beta_1 pvalue:	5.081103394387796e-88
SE:		0.03873341621263941
Beta_0 SE:	0.562627354988433</code></pre>
<ul>
<li>MISSING
<ul>
<li>Beta_0 pvalue</li>
<li>tvalues for both Betas,</li>
</ul></li>
</ul>
<hr />
</section>
<section id="numpy" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Numpy</h1>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>Betas, RSS, Rank, SingVals <span class="op">=</span>np.linalg.lstsq(X,Y,rcond<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="ss">Beta_1, Beta_0: </span><span class="sc">{</span>[i[<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> Betas]<span class="sc">}</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="ss">RSS: </span><span class="sc">{</span>RSS<span class="sc">}</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="ss">RSE : </span><span class="sc">{</span>(RSS<span class="op">/</span>(X.shape[<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span>)) <span class="op">**</span> (<span class="fl">0.5</span>)<span class="sc">}</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="ss">Rank: </span><span class="sc">{</span>Rank<span class="sc">}</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="ss">SingularVal: </span><span class="sc">{</span>SingVals<span class="sc">}</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span>)</span></code></pre></div>
<pre><code>Beta_1, Beta_0: [-0.95004934, 34.55384]
RSS: [19472.38]
RSE : [6.21576]
Rank: 2
SingularVal: [327.33368   11.027905]</code></pre>
<hr />
</section>
<section id="torch" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Torch</h1>
<section id="simple-linreg" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">7.1</span> Simple LinReg</h2>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>Xtensor <span class="op">=</span> torch.from_numpy(X)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>Ytensor <span class="op">=</span> torch.from_numpy(Y)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.linalg.lstsq(Xtensor, Ytensor).solution</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(W)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># print(X[:,[1]])</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>Xlayer <span class="op">=</span> torch.from_numpy(X[:,[<span class="dv">0</span>]])</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>hidlayer <span class="op">=</span> torch.from_numpy(X[:,[<span class="dv">1</span>]])</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>Ytensor <span class="op">=</span> torch.from_numpy(Y)</span></code></pre></div>
<pre><code>tensor([[-0.9500],
        [34.5538]])</code></pre>
</section>
<section id="layer-nn" class="level2" data-number="7.2">
<h2 data-number="7.2"><span class="header-section-number">7.2</span> 1-layer NN</h2>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, optim</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython <span class="im">import</span> display</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>random.seed(seed)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>inputDim <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>outputDim <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>hiddenDim <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>lambda_l2 <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    nn.Linear(inputDim, hiddenDim),</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    nn.Linear(hiddenDim, outputDim)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>model.to(device) <span class="co"># Convert to CUDA</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a><span class="co"># nn package also has different loss functions.</span></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a><span class="co"># we use MSE loss for our regression task</span></span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a><span class="co"># we use the optim package to apply</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a><span class="co"># stochastic gradient descent for our parameter updates</span></span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate, weight_decay<span class="op">=</span>lambda_l2) <span class="co"># built-in L2</span></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Feed forward to get the logits</span></span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(Xlayer)</span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the loss (MSE)</span></span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(y_pred, Ytensor)</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;[EPOCH]: </span><span class="sc">%i</span><span class="st">, [LOSS or MSE]: </span><span class="sc">%.6f</span><span class="st">&quot;</span> <span class="op">%</span> (t, loss.item()))</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>    display.clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># zero the gradients before running</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the backward pass.</span></span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass to compute the gradient</span></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># of loss w.r.t our learnable params. </span></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update params</span></span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div>
<pre><code>[EPOCH]: 999, [LOSS or MSE]: 39.632763</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(Xlayer.data.cpu().numpy(), Ytensor.data.cpu().numpy())</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plt.plot(Xlayer.data.cpu().numpy(), y_pred.data.cpu().numpy(), <span class="st">'r-'</span>, lw<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'equal'</span>)<span class="op">;</span></span></code></pre></div>
<figure>
<img src="../images/2016-07-07-LinReg/output_46_0.png" alt="png" /><figcaption aria-hidden="true">png</figcaption>
</figure>
</section>
<section id="layer-nn-class-style" class="level2" data-number="7.3">
<h2 data-number="7.3"><span class="header-section-number">7.3</span> 1 layer NN Class style</h2>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.autograd <span class="im">import</span> Variable</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>Xm <span class="op">=</span> np.delete(X,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>Xtensor <span class="op">=</span> torch.from_numpy(Xm)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>Ytensor <span class="op">=</span> torch.from_numpy(Y)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>numHiddenlayer <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">#In pytorch we dont need to artifically add a hidden layer column to our X input</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearRegressionModel(torch.nn.Module):</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(LinearRegressionModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linearA <span class="op">=</span> torch.nn.Linear(Xtensor.shape[<span class="dv">1</span>], numHiddenlayer,bias<span class="op">=</span><span class="va">True</span>)  <span class="co"># One in and one out</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linearB <span class="op">=</span> torch.nn.Linear(numHiddenlayer,Ytensor.shape[<span class="dv">1</span>])</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">#y_pred = F.relu(self.linear(x))</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> <span class="va">self</span>.linearB(<span class="va">self</span>.linearA(x))</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">#y_pred = self.linearA(x)</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y_pred</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co"># our model</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>our_model <span class="op">=</span> LinearRegressionModel()</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="co">#EXTRREMELY SENSITIVE TO LEARNING RATE 1e-4 is sweetspot</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> (<span class="fl">1e-4</span>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(our_model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a><span class="co">#w,b = our_model.parameters()</span></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f&quot;initial: {w,b}&quot;)</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>bestloss <span class="op">=</span> <span class="dv">99999</span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>bestepoch <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4000</span>): <span class="co">#EXTREMELY SENSITIVE TO EPOCH, 50000 is sweetspot</span></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>    epoch <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert numpy array to torch Variable</span></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">#inputs = torch.from_numpy(p).requires_grad_()</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">#labels = torch.from_numpy(q)</span></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Clear gradients w.r.t. parameters</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad() </span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward to get output</span></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> our_model(Xtensor)</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate Loss</span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(outputs, Ytensor)</span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>    currloss <span class="op">=</span> loss.item()</span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> currloss <span class="op">&lt;</span> bestloss:</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>        bestloss <span class="op">=</span> currloss</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>        bestepoch <span class="op">=</span> epoch</span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if currloss - bestloss &gt; 200 and epoch &gt; 50:</span></span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(&quot;boom&quot;)</span></span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Getting gradients w.r.t. parameters</span></span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Updating parameters</span></span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'epoch </span><span class="sc">{}</span><span class="st">, loss </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(epoch, loss.item()))</span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'epoch </span><span class="sc">{}</span><span class="st">, loss </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(bestepoch, bestloss))</span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a><span class="co">#w,b = our_model.parameters()</span></span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a><span class="co">#print(w,b)</span></span></code></pre></div>
<pre><code>epoch 4000, loss 38.486961364746094
epoch 4000, loss 38.486961364746094</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>w,b <span class="op">=</span> our_model.parameters()</span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w,b)</span></code></pre></div>
<pre><code>Parameter containing:
tensor([[-4.9096]], requires_grad=True) Parameter containing:
tensor([0.5459], requires_grad=True)</code></pre>
</section>
</section>
<section id="numpy-vs-scipy-vs-torch-vs-statsmodels" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Numpy vs Scipy vs Torch vs Statsmodels</h1>
<p>From the above linear regression methods we can see that Numpy, Scipy, Torch lack the fine details that Statsmodel provide.<br />
Statsmodel provide details that resemble R much more closely.</p>
</section>

        </div>
        <div id="footer">
            <div class="flex-container" style="display:flex; justify-content: space-between;">
                <div>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
                </div>
   
            <div xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://userjy.github.io/">Jason's Notes</a> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://userjy.github.io/">Jason Yang</a> is licensed under <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC-ND 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nd.svg?ref=chooser-v1"></a></div>
  
            </div>
        </div>
        
    </body>
    <footer>
        <!-- CODE TAB START -->
        <script>
                    //Structure:
                    // Codeblock < Group/Grouplabel/subgrp < datagroupSet < allblocks

            const AllBlocksPre = document.querySelectorAll("[data-group]");
            const AllBlocks = [...AllBlocksPre]; //gets all codeblocks w/ and w/o group label
            const getUniqueSet = (TargetSet,dataAttr) => {
                //gets the set of attributes of an array of codeblocks aka TargetSet
                const temp = TargetSet.map((e) => (e.getAttribute(dataAttr))); 
                const temp2 = temp.filter((a)=>a); //remove nulls
                return [...new Set(temp2)];
            } 
            const datagroupSet = getUniqueSet(AllBlocks,"data-group") //remove nulls

            const getCodeBlocks = (datagroup) => {
                //return list of glabels CodeBlocks associated to a single group 
                return AllBlocks.filter((dataglabelBlock)=>(dataglabelBlock.getAttribute("data-group") === datagroup));
            }

            const showBlocks = (dataglabeltxt,datagroupCodeBlocks) => {
                const selectedglabelGroup = datagroupCodeBlocks.filter((SingleBlock)=>(SingleBlock.getAttribute("data-glabel") === dataglabeltxt))
                const NONselectedglabelGroup = datagroupCodeBlocks.filter((SingleBlock)=>(SingleBlock.getAttribute("data-glabel") !== dataglabeltxt))
                selectedglabelGroup.map((SingleBlock) => (SingleBlock.style.display="block"));
                (NONselectedglabelGroup || []).map((SingleBlock) => (SingleBlock.style.display="none"));
            }
            const mkBtn = (dataglabeltxt,datagroupCodeBlocks,showfunc) => {
                const newbutton = document.createElement("button");
                newbutton.textContent = dataglabeltxt;
                newbutton.addEventListener('click', ()=>{
                    showfunc(dataglabeltxt,datagroupCodeBlocks);
                });
                return newbutton;
            }
            const showAll = (datagroup) => {
                //make all codeblocks visible
            datagroup.map((e)=>(e.style.display="block"));
            }

            const buildCodeTab = (datagroupCodeBlocks) => {
                const leader = datagroupCodeBlocks[0]; //get the leader codeblock of a group of codeblock
                const setglabelstxt = getUniqueSet(datagroupCodeBlocks,"data-glabel"); 
                setglabelstxt.map((singleglabeltxt)=>{
                    const btn = mkBtn(singleglabeltxt,datagroupCodeBlocks,showBlocks)
                    leader.insertAdjacentElement("beforebegin",btn);

                })
                
                //make showAll button START
                const btnShowAll = document.createElement("button");
                btnShowAll.textContent = "All";
                btnShowAll.addEventListener('click',(e)=>(showAll(datagroupCodeBlocks)));
                leader.insertAdjacentElement('beforebegin', btnShowAll);
                //make showAllbutton END
            }

            //below code is performing actual behavior, the above code are just functions
            datagroupSet.map((datagroup) => {
                const groupOfCodeblocks = getCodeBlocks(datagroup);
                buildCodeTab(groupOfCodeblocks);
                const firsttab = groupOfCodeblocks[0]
                showBlocks(firsttab.getAttribute("data-glabel"),groupOfCodeblocks);
       
            })
        </script>
        <!-- CODE TAB END ---->

        <!-- MATH JAX START -------------------------------------- -->
        <script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.min.js">
        </script>
        <!-- MATH JAX END ----------------------------------------- -->

        <!-- RAILROAD START -------------------------------------- -->
        <script type="module">
            import rr,* as rrClass from "/lib/railroad/railroad.js";
            Object.assign(window,rr)
            window.rrOptions = rrClass.Options;
            document.addEventListener('DOMContentLoaded',()=>{ReplaceDivWithSvg()},false)
            const ReplaceDivWithSvg = () =>  {
                for (const railroadelem of document.getElementsByClassName("rroad") ){
                railroadelem.innerHTML = eval(railroadelem.innerText.trim()+".toString()")
                }
            }
        </script>
        

        <link rel="stylesheet" href="../lib/railroad/railroad-diagrams.css">
        <!-- RAILROAD END ----------------------------------------- -->
    </footer>
</html>
